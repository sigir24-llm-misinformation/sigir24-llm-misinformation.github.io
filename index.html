<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ACL 2023 Tutorial: Retrieval-based LMs and Applications</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
              <span style="font-size: 80%">SIGIR 2024 Tutorial:</span><br />
              Preventing and Detecting Misinformation Generated by Large Language Models
            </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <table>
            <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="33%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_aiwei.jpeg"></td>
                <td width="33%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_shengqiang.png"></td>
                <td width="33%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_xuming.jpeg"></td>
                <!-- <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_danqi.png"></td> -->
            </tr>
              <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="33%" style="text-align: center"><a href="https://exlaw.github.io/" style="border-radius: 50%">Aiwei Liu</a><sup>1</sup>,</td>
                <td width="33%" style="text-align: center"><a href="https://sheng-qiang.github.io/" style="border-radius: 50%">Qiang Sheng</a><sup>2</sup>,</td>
                <td width="33%" style="text-align: center"><a href="https://xuminghu.github.io/" style="border-radius: 50%">Xuming Hu</a><sup>3</sup></td>
              </tr>
            </table>
              <!-- <a href="https://akariasai.github.io/">Akari Asai</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://shmsw25.github.io/">Sewon Min</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~zzhong/">Zexuan Zhong</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a><sup>2</sup>, -->
            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Institute of Computing Technology, Chinese Academy of Sciences</span>
            <span class="author-block"><sup>3</sup>The Hong Kong University of Science and Technology (Guangzhou)</span>
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <b>Tuesday July 14 South American B 13:30 - 17:00 (EST) @ Washington, D.C.</b>
          </div>
          

          <div class="is-size-5 publication-authors">
            <!--
            Zoom link available on <a href="https://underline.io/events/395/sessions?eventSessionId=15330&searchGroup=lecture" target="_blank">Underline</a>
            -->
            Visit <a target="_blank" href="https://XXXXXX">this link</a>
            for the Zoom recording of the tutorial
          </div>
          <!--<div class="is-size-6 publication-authors">
            For those who have not registered to ACL: we will release video recordings after the tutorial
          </div>
          <br />-->
          <!-- <div class="is-size-5 publication-authors">
            QnA: <a href="https://tinyurl.com/retrieval-lm-tutorial" target="_blank"><b>tinyurl.com/retrieval-lm-tutorial</b></a>
          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <p>
            As large language models (LLMs) become increasingly capable and widely deployed, the risk of them generating misinformation poses a critical challenge. Misinformation from LLMs can take various forms, from factual errors due to hallucination to intentionally deceptive content, and can have severe consequences in high-stakes domains.
          </p>
          <p>
            This tutorial covers comprehensive strategies to prevent and detect misinformation generated by LLMs. We first introduce the types of misinformation LLMs can produce and their root causes. We then explore two broad categories:
          </p>
          <p>
            Preventing misinformation generation:
          </p>
          <ul>
            <li>
              a) Enhancing LLM Knowledge:
              <ul>
                <li>[Internal Knowledge] Constructing more truthful datasets</li>
                <li>[Internal Knowledge] LLM knowledge editing</li>
                <li>[External Knowledge] Retrieval augmented generation</li>
              </ul>
            </li>
            <li>
              b) Enhancing Knowledge Inference in LLMs:
              <ul>
                <li>Factual decoding method</li>
                <li>Factual alignment</li>
                <li>Adversarial training</li>
              </ul>
            </li>
            <li>
              c) Promoting Ethical Values in LLMs:
              <ul>
                <li>Safety alignment</li>
              </ul>
            </li>
          </ul>
          <p>
            Detecting misinformation after generation, including:
          </p>
          <ul>
            <li>
              a) LLM-Generated Text Detection:
              <ul>
                <li>Watermarking based detection</li>
                <li>Post-generation detection</li>
              </ul>
            </li>
            <li>
              b) Misinformation Detection:
              <ul>
                <li>General misinformation detection</li>
                <li>LLM-generated misinformation detection</li>
              </ul>
            </li>
          </ul>
          <p>
            We also discuss the challenges and limitations of detecting LLM-generated misinformation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Schedule</h2>
        <p>
          Our tutorial will be held on July 14 (all the times are based on EST = Washington local time).
          <!-- <em>Slides may be subject to updates.</em> -->
        </p>

        <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg">
          <thead>
            <tr>
              <th class="tg-0pky">Time</th>
              <th class="tg-0lax">Section</th>
              <th class="tg-0lax">Presenter</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">13:00—13:45</td>
              <td class="tg-0lax">Section 1: Overview of LLM Generated Misinformation</td>
              <td class="tg-0lax"><s>Xuming (Visa Issue)</s> Aiwei</td>
            </tr>
            <tr>
              <td class="tg-0lax">13:45—14:15</td>
              <td class="tg-0lax">Section 2: [Preventing Misinformation] Enhancing LLM Knowledge </td>
              <td class="tg-0lax"><s>Xuming (Visa Issue)</s> Aiwei</td>
            </tr>
            <tr>
              <td class="tg-0lax">14:15—14:45</td>
              <td class="tg-0lax">Section 3: [Preventing Misinformation] Enhancing Knowledge Inference in LLMs </td>
              <td class="tg-0lax">Aiwei</td>
            </tr>
            <tr>
              <td class="tg-0lax">14:45—14:55</td>
              <td class="tg-0lax">Section 4: [Preventing Misinformation] Promoting Ethical Values in LLMs </td>
              <td class="tg-0lax">Aiwei</td>
            </tr>
            <tr>
              <td class="tg-0lax">14:55—15:00</td>
              <td class="tg-0lax">Q & A Session I</td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">15:00—15:30</td>
              <td class="tg-0lax">Coffee break</td>
              <td class="tg-0lax"></td>
            </tr>
            <tr>
              <td class="tg-0lax">15:30—15:50</td>
              <td class="tg-0lax">Section 5: [Detecting Misinformation] Watermarking Based Detection </td>
              <td class="tg-0lax">Aiwei</td>
            </tr>
            <tr>
              <td class="tg-0lax">15:50—16:15</td>
              <td class="tg-0lax">Section 6: [Detecting Misinformation] Post-Generation Detection </td>
              <td class="tg-0lax">Qiang</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:15—16:30</td>
              <td class="tg-0lax">Section 7: [Detecting Misinformation] General Misinformation Detection</td>
              <td class="tg-0lax">Qiang</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:30—16:45</td>
              <td class="tg-0lax">Section 8: [Detecting Misinformation] LLM-Generated Misinformation Detection </td>
              <td class="tg-0lax">Qiang</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:45—16:50</td>
              <td class="tg-0lax">Section 9: Conclusion and Discussion </td>
              <td class="tg-0lax">Qiang</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:50—17:00</td>
              <td class="tg-0lax">Q & A Session II</td>
              <td class="tg-0lax"></td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reading List</h2>

        <!-- <p><b>Bold papers</b> are discussed in detail during our tutorial.</p> -->

        <br />

        <h3 class="title is-5">Section 1: Overview of LLM Generated Misinformation</h3>

        <ul>
          <li><a href="https://arxiv.org/pdf/2402.06196">Large Language Models: A Survey</a> (Minaee, Shervin, et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2206.07682">Emergent abilities of large language models</a> (Wei, Jason, et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2308.05374">Trustworthy LLMs: A survey and guideline for evaluating large language models' alignment</a> (Liu et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2309.13788">Can llm-generated misinformation be detected?</a> (Chen et al., 2023)</li>
        </ul>
        
        <br />
        
        
        <h3 class="title is-5">Section 2: [Preventing Misinformation] Enhancing LLM Knowledge </h3>
        <ul>
        <li><a href="https://arxiv.org/pdf/2305.13169">A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity</a> (Longpre, Shayne, et al., 2023)</li>
        <li><a href="https://arxiv.org/pdf/2306.11644">Textbooks Are All You Need</a> (Gunasekar et al., 2023)</li>
        <li><a href="https://arxiv.org/pdf/2305.13172">Editing Large Language Models: Problems, Methods, and Opportunities</a> (Yao et al., 2023)</li>
        <li><a href="https://arxiv.org/pdf/2305.17553">Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark</a> (Hoelscher et al., 2023)</li>
        <li><a href="https://arxiv.org/pdf/2104.08696">Knowledge neurons in pretrained transformers</a> (Dai, et al., 2022)</li>
        <li><a href="https://arxiv.org/pdf/2206.06520">Memory-based model editing at scale</a> (Mitchell et al., 2022)</li>
        <li><a href="https://arxiv.org/pdf/2312.10997">Retrieval-Augmented Generation for Large Language Models: A Survey</a> (Gao et al., 2024)</li>
        <li><a href="https://arxiv.org/pdf/2305.14283">Query Rewriting for Retrieval-Augmented Large Language Models</a> (Ma et al., 2023)</li>
        <li><a href="https://arxiv.org/pdf/2305.06983">Active Retrieval Augmented Generation</a> (Jiang et al., 2023)</li>
        </ul>
        <br />

        <h3 class="title is-5">Section 3: [Preventing Misinformation] Enhancing Knowledge Inference in LLMs</h3>

        <ul>
          <li><a href="https://arxiv.org/pdf/2206.04624">Factuality Enhanced Language Models for Open-Ended Text Generation</a> (Lee et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2306.03341">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a> (Li et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2309.03883">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a> (Chuang et al., 2023)</li>

          <li><a href="https://arxiv.org/pdf/2403.05612">Unfamiliar Finetuning Examples Control How Language Models Hallucinate</a> (Kang et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2311.08401">Fine-tuning Language Models for Factuality</a> (Tian et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2312.07000">Alignment for Honesty </a> (Yang et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2405.01525">FLAME : Factuality-Aware Alignment for Large Language Models</a> (Lin et al., 2024)</li>
          <li><a href="https://aclanthology.org/2024.findings-naacl.85.pdf">ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks</a> (Yu et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2310.01469">LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples</a> (Yao et al., 2023)</li>
        </ul>

        <br />

        <h3 class="title is-5">Section 4: [Preventing Misinformation] Promoting Ethical Values in LLMs</h3>

        <ul>
          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4dbb61cb68671edc4ca3712d70083b9f-Paper-Datasets_and_Benchmarks.pdf">Beavertails: Towards improved safety alignment of llm via a human-preference dataset</a> (Ji et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2306.03341">Refusal in Language Models Is Mediated by a Single Direction </a> (Arditi et al., 2024)</li>
        </ul>

        <br />

       

        <h3 class="title is-5">Section 5: [Detecting Misinformation] Watermarking Based Detection</h3>

        <ul>
          <li><a href="https://arxiv.org/pdf/2401.11817">Hallucination is Inevitable: An Innate Limitation of Large Language Models</a> (Xu et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2303.07205">The Science of Detecting LLM-Generated Texts </a> (Tang et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2312.07913">A Survey of Text Watermarking in the Era of Large Language Models</a> (Liu et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2301.10226">A Watermark for Large Language Models</a> (Kirchenbauer et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2310.06356">A Semantic Invariant Robust Watermark for Large Language Models</a> (Liu et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2310.10669">Unbiased Watermark for Large Language Models</a> (Hu et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2405.10051">MarkLLM: An Open-Source Toolkit for LLM Watermarking</a> (Pan et al., 2024)</li>
        </ul>

        <br />

        <h3 class="title is-5">Section 6: [Detecting Misinformation] Post-Generation Detection</h3>

        <ul>
          <li><a href="https://arxiv.org/pdf/2403.01152">A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization</a> (Kumarage1 et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/1906.04043">GLTR: Statistical Detection and Visualization of Generated Text</a> (Gehrmann et al., 2019)</li>
          <li><a href="https://arxiv.org/pdf/2301.11305">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</a> (Mitchell et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2306.05540">DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text</a> (Su et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2305.17359">DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text</a> (Yang et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2304.14072">Origin Tracing and Detecting of LLMs </a> (Li et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2305.15004">LLMDet: A Large Language Models Detection Tool</a> (Wu et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2303.07205">The science of detecting llm-generated text</a> (Tang et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2401.06712">Few-Shot Detection of Machine-Generated Text using Style Representations</a> (Soto et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2212.10341">COCO: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning</a> (Liu et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2401.12970">Raidar: geneRative AI Detection viA Rewriting</a> (Mao et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2305.12519">DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection
          </a> (Yu et al., 2023)</li>
        </ul>

        <br />

        <h3 class="title is-5">Section 7: [Detecting Misinformation] General Misinformation Detection</h3>

        <ul>
          <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/30214">Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection</a> (Hu et al., 2024)</li>
          <li><a href="https://dl.acm.org/doi/abs/10.1145/3589334.3645471">Explainable Fake News Detection with Large Language Model via Defense Among Competing Wisdom  </a> (Wang et al., 2019)</li>
          <li><a href="https://arxiv.org/pdf/2405.16631">Let Silence Speak: Enhancing Fake News Detection with Generated Comments from Large Language Models
            </a> (Nan et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2307.13528">FACTOOL: Factuality Detection in Generative AI: A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios</a> (Chern et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2305.12744">Fact-Checking Complex Claims with Program-Guided Reasoning</a> (Pan et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2304.14072">TELLER: A Trustworthy Framework For Explainable, Generalizable and Controllable Fake News Detection
             </a> (Liu et al., 2024)</li>
        </ul>

        <br />

        <h3 class="title is-5">Section 8: [Detecting Misinformation] LLM-Generated Misinformation Detection</h3>

        <ul>
          <li><a href="https://arxiv.org/pdf/2303.08896">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection
            for Generative Large Language Models</a> (Manakul et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2403.02889">In Search of Truth: An Interrogation Approach to Hallucination Detection</a> ( Yehuda et al., 2024)</li>
          <li><a href="https://arxiv.org/pdf/2304.13734">The Internal State of an LLM Knows When It’s Lying
            </a> (Azaria et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2310.10830">Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks</a> (Wu et al., 2023)</li>
          <!-- <li><a href="https://arxiv.org/pdf/2305.12744"><b>Fact-Checking Complex Claims with Program-Guided Reasoning</b></a> (Pan et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2304.14072"><b>TELLER: A Trustworthy Framework For Explainable, Generalizable and Controllable Fake News Detection
             </b></a> (Liu et al., 2024)</li> -->
        </ul>

        <br />


      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{10.1145/3626772.3661377,
      author = {Liu, Aiwei and Sheng, Qiang and Hu, Xuming},
      title = {Preventing and Detecting Misinformation Generated by Large Language Models},
      year = {2024},
      isbn = {9798400704314},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3626772.3661377},
      doi = {10.1145/3626772.3661377},
      pages = {3001–3004},
      numpages = {4},
      keywords = {hallucination, large language models, misinformation},
      location = {Washington DC, USA},
      series = {SIGIR '24}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/sigir24-llm-misinformation" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
